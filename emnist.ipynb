{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handwritten Character Recognition using EMNIST Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the main libraries needed for the project <br>\n",
    "emnist -- dataset <br>\n",
    "tensorflow -- libraries useful for machine learning <br>\n",
    "matplotlib -- data visualization <br>\n",
    "PIL -- image grabbing and manipulation <br>\n",
    "keras -- api for machine learning <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from emnist import list_datasets\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import ImageGrab, Image\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>List the datasets available for the EMNIST</h1><br>\n",
    "Balanced -- Alphanumeric database containing 814,255 units, 697,932 train, 116,323 test. 47 different classes to identify characters. Certain alphabets have been combined who have similar-looking UPPER and LOWER case image. Each class contains an equal amount of data to be used for training/testing<br>\n",
    "ByClass -- Alphanumeric database containing 814,255 units, 697,932 train, 116,323 test. 62 different classes to identify characters, 10 for digits, 26 for upper-case, 26 for lower-case. Certain classes have more units due to frequeuncy of usage in the english language<br>\n",
    "ByMerge -- Alphanumeric database containing 814,255 units, 697,932 train, 116,323 test. 47 different classes to identify characters. Certain alphabets have been combined who have similar-looking UPPER and LOWER case image. Certain classes have more units due to frequeunct of usage in the english language <br>\n",
    "Digits -- Larger database for digits. 280,000 units, 240,000 train, 40,000 test <br>\n",
    "Letters -- Database for only alphabetical characters. 103,600 units, 88,800 train, 14,800 test <br>\n",
    "MNIST -- Default Database for digits. 70,000 units, 60000 train, 10,000 test <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['balanced', 'byclass', 'bymerge', 'digits', 'letters', 'mnist']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from emnist import extract_training_samples\n",
    "from emnist import extract_test_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = extract_training_samples('balanced')  # split the train/test data x_train = model y_train = val\n",
    "x_test, y_test = extract_test_samples('balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(112800, 28, 28) (112800,) (18800, 28, 28) (18800,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_train[0], cmap=plt.get_cmap('gray'))\n",
    "plt.title(y_train[0])\n",
    "plt.show()\n",
    "# index 0 of y_train represents the 45th value of the 47 classes, in this case lowercase r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)  # reshape data to 28x28 in 1 dimension\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "x_train = x_train.astype('float32')  # make the data include float values, and constrain the values between0 to 1\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complex model with several layers to filter and classiy the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3525/3525 [==============================] - 13s 4ms/step - loss: 0.8059 - accuracy: 0.7481\n",
      "Epoch 2/10\n",
      "3525/3525 [==============================] - 13s 4ms/step - loss: 0.4682 - accuracy: 0.8380\n",
      "Epoch 3/10\n",
      "3525/3525 [==============================] - 13s 4ms/step - loss: 0.4022 - accuracy: 0.8566\n",
      "Epoch 4/10\n",
      "3525/3525 [==============================] - 13s 4ms/step - loss: 0.3622 - accuracy: 0.8690\n",
      "Epoch 5/10\n",
      "3525/3525 [==============================] - 13s 4ms/step - loss: 0.3354 - accuracy: 0.8764\n",
      "Epoch 6/10\n",
      "3525/3525 [==============================] - 13s 4ms/step - loss: 0.3131 - accuracy: 0.8838\n",
      "Epoch 7/10\n",
      "3525/3525 [==============================] - 13s 4ms/step - loss: 0.2976 - accuracy: 0.8871\n",
      "Epoch 8/10\n",
      "3525/3525 [==============================] - 13s 4ms/step - loss: 0.2790 - accuracy: 0.8929\n",
      "Epoch 9/10\n",
      "3525/3525 [==============================] - 13s 4ms/step - loss: 0.2655 - accuracy: 0.8967\n",
      "Epoch 10/10\n",
      "3525/3525 [==============================] - 13s 4ms/step - loss: 0.2549 - accuracy: 0.9003\n",
      "588/588 [==============================] - 1s 2ms/step - loss: 0.3977 - accuracy: 0.8695\n",
      "INFO:tensorflow:Assets written to: sample.model\\assets\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_2 (Conv2D)            (None, 26, 26, 6)         60        \n",
      "_________________________________________________________________\n",
      "average_pooling2d_2 (Average (None, 13, 13, 6)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 11, 11, 16)        880       \n",
      "_________________________________________________________________\n",
      "average_pooling2d_3 (Average (None, 5, 5, 16)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 188)               75388     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 94)                17766     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 47)                4465      \n",
      "=================================================================\n",
      "Total params: 98,559\n",
      "Trainable params: 98,559\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Conv2D(filters=6, kernel_size=(3, 3), activation='relu', input_shape=(28,28,1)),\n",
    "  tf.keras.layers.AveragePooling2D(),\n",
    "  tf.keras.layers.Conv2D(filters=16, kernel_size=(3, 3), activation='relu'),\n",
    "  tf.keras.layers.AveragePooling2D(),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(188, activation='relu'),\n",
    "  tf.keras.layers.Dense(94, activation='relu'),\n",
    "  tf.keras.layers.Dense(47, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=10)\n",
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "model.save('sample.model')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic model using only dense layers to filter the data and classify the data. Each layer categorizes the data, aiming for more accuracy each time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = tf.keras.models.Sequential([\n",
    "#   tf.keras.layers.Flatten(input_shape=(28, 28, 1)),\n",
    "#   tf.keras.layers.Dense(512, activation='relu'),\n",
    "#   tf.keras.layers.Dense(256, activation='relu'),\n",
    "#   tf.keras.layers.Dense(188, activation='relu'),   \n",
    "#   tf.keras.layers.Dense(94, activation='relu'),\n",
    "#   tf.keras.layers.Dense(47, activation='softmax')\n",
    "# ])\n",
    "\n",
    "# model.compile(optimizer='adam',\n",
    "#               loss='sparse_categorical_crossentropy',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# model.fit(x_train, y_train, epochs=10)\n",
    "# loss, accuracy = model.evaluate(x_test, y_test)\n",
    "# model.save('sample.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the mapping text file given with the dataset to classify the images based on their index.<br> Each mapping index corresponds to an ASCII value (0-9, A-Z, a, b, d, e, f, g, h, n, q, r, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 48  49  50  51  52  53  54  55  56  57  65  66  67  68  69  70  71  72\n",
      "  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90\n",
      "  97  98 100 101 102 103 104 110 113 114 116]\n",
      "{0: '0', 1: '1', 2: '2', 3: '3', 4: '4', 5: '5', 6: '6', 7: '7', 8: '8', 9: '9', 10: 'A', 11: 'B', 12: 'C', 13: 'D', 14: 'E', 15: 'F', 16: 'G', 17: 'H', 18: 'I', 19: 'J', 20: 'K', 21: 'L', 22: 'M', 23: 'N', 24: 'O', 25: 'P', 26: 'Q', 27: 'R', 28: 'S', 29: 'T', 30: 'U', 31: 'V', 32: 'W', 33: 'X', 34: 'Y', 35: 'Z', 36: 'a', 37: 'b', 38: 'd', 39: 'e', 40: 'f', 41: 'g', 42: 'h', 43: 'n', 44: 'q', 45: 'r', 46: 't'}\n"
     ]
    }
   ],
   "source": [
    "mapping = np.loadtxt('emnist-balanced-mapping.txt',dtype=int, usecols=(1), unpack=True)\n",
    "print(mapping)\n",
    "char_labels={}\n",
    "for i in range(47):\n",
    "    char_labels[i] = chr(mapping[i])\n",
    "print(char_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only need to run bottom line inorder to load the model. Do not need to train the model each time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from PIL import ImageGrab, Image\n",
    "import numpy as np\n",
    "model = keras.models.load_model('emnist.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image testing with drawn images from tkinter gui. Each image is unique, 0-9, A-Z, then lower case letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "L\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "G\n",
      "7\n",
      "8\n",
      "9\n",
      "A\n",
      "8\n",
      "C\n",
      "D\n",
      "E\n",
      "F\n",
      "G\n",
      "H\n",
      "I\n",
      "J\n",
      "K\n",
      "L\n",
      "M\n",
      "N\n",
      "0\n",
      "P\n",
      "Q\n",
      "R\n",
      "S\n",
      "T\n",
      "U\n",
      "V\n",
      "W\n",
      "X\n",
      "Y\n",
      "Z\n",
      "a\n",
      "b\n",
      "d\n",
      "E\n",
      "F\n",
      "g\n",
      "h\n",
      "M\n",
      "q\n",
      "t\n",
      "T\n"
     ]
    }
   ],
   "source": [
    "for x in range(0,47):\n",
    "    img = Image.open(f'images/{x}.png')\n",
    "    img = np.array(img)\n",
    "    img = np.invert(np.array([img]))\n",
    "    img = img.reshape(1,28,28,1)\n",
    "    img = img/255\n",
    "    predict = model.predict(img)  # identify the class which resonates highest with the image\n",
    "    print(char_labels[np.argmax(predict)])\n",
    "#     Testing with 10 pixel lines"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gputensor 3.7",
   "language": "python",
   "name": "gputensor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
